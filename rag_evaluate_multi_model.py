#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€é”® RAG è¯„ä¼°è„šæœ¬ï¼ˆå·²å¢å¼ºï¼‰ï¼š
- å…¼å®¹æœ¬åœ° Baichuan-7Bï¼ˆåŠç²¾åº¦ + åˆ†æ‰¹åŠ è½½ + eval æ¨¡å¼ï¼‰
- æ›´å¥å£®çš„ LLM-based è¯„åˆ†è§£æï¼ˆå®¹é”™ JSONï¼‰
- matplotlib é£æ ¼å›é€€å¤„ç†
- æ¯æ¬¡ç”Ÿæˆåå°½é‡å›æ”¶æ˜¾å­˜ï¼Œé‡åˆ° OOM æœ‰ç®€è¦é™çº§ç­–ç•¥

ä½¿ç”¨ï¼špython rag_evaluate_multi_model.py
"""
import os
import sys
import json
import time
import re
from pathlib import Path
from tqdm.auto import tqdm
import warnings

# ä¾èµ–å¯¼å…¥ï¼ˆå®¹é”™ï¼‰
missing = []
try:
    import pandas as pd
except Exception:
    pd = None
    missing.append("pandas")
try:
    import numpy as np
except Exception:
    np = None
    missing.append("numpy")
try:
    from sentence_transformers import SentenceTransformer, util
except Exception:
    SentenceTransformer = None
    util = None
    missing.append("sentence-transformers")

try:
    import faiss
except Exception:
    faiss = None

try:
    import sacrebleu
except Exception:
    sacrebleu = None

try:
    from rouge_score import rouge_scorer
except Exception:
    rouge_scorer = None

try:
    import matplotlib.pyplot as plt
except Exception:
    plt = None

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
except Exception:
    AutoTokenizer = None
    AutoModelForCausalLM = None

import torch

# ========== é…ç½® ==========
BAICHUAN_PATH = "/root/Agent/baichuan-7b"   # æœ¬åœ° Baichuan æ¨¡å‹ç›®å½•
EMBED_DIRS = [
    "./e5-base-v2",
    "./bge-large-zh",
    "./gte-large"
]
DOCUMENTS_CSV = "./documents.csv"
TEST_FILE = "./rag_results.csv"
OUTPUT_DIR = "./results"
TOP_K = 5
EMBED_BATCH = 64
DEVICE = "cuda"  # or "cpu"
# ========================

os.makedirs(OUTPUT_DIR, exist_ok=True)
charts_dir = Path(OUTPUT_DIR) / "charts_paper"
charts_dir.mkdir(parents=True, exist_ok=True)

if missing:
    print("ç¼ºå°‘ä¾èµ–åŒ…ï¼š", missing)
    print("å»ºè®®å…ˆå®‰è£…ï¼špip install " + " ".join(missing))
    # ä»ç„¶å°è¯•è¿è¡Œï¼ˆéƒ¨åˆ†åŠŸèƒ½ä¼šæŠ¥é”™ï¼‰

# ---- åŠ è½½æ–‡æ¡£åº“ ----
print("ğŸ“Œ åŠ è½½æ–‡æ¡£åº“...")
if not Path(DOCUMENTS_CSV).exists():
    raise FileNotFoundError(f"documents CSV æœªæ‰¾åˆ°: {DOCUMENTS_CSV}")

df_docs = pd.read_csv(DOCUMENTS_CSV)
if "doc_id" not in df_docs.columns or "text" not in df_docs.columns:
    if "id" in df_docs.columns and "content" in df_docs.columns:
        df_docs = df_docs.rename(columns={"id":"doc_id","content":"text"})
    else:
        raise ValueError("documents.csv éœ€è¦åŒ…å« 'doc_id' ä¸ 'text' åˆ—")

doc_texts = df_docs["text"].fillna("").tolist()
doc_ids = df_docs["doc_id"].tolist()
print(f"æ–‡æ¡£æ•°é‡ï¼š{len(doc_texts)}")

# ---- è¯»æµ‹è¯•é›† ----
if not Path(TEST_FILE).exists():
    raise FileNotFoundError(f"æµ‹è¯•é›†æ–‡ä»¶æœªæ‰¾åˆ°: {TEST_FILE}")
df_test = pd.read_csv(TEST_FILE).fillna("")
if "question" not in df_test.columns or "gold_answer" not in df_test.columns:
    raise ValueError("æµ‹è¯•é›†æ–‡ä»¶éœ€è¦åŒ…å« 'question' ä¸ 'gold_answer' åˆ—")
tests = df_test.to_dict(orient="records")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(tests)}")

# ---- åŠ è½½ Baichuan ä½œä¸ºè¯„ä¼°ä¸ç”Ÿæˆ LLMï¼ˆç”¨ä½œRAGçš„ç”Ÿæˆä¸LLMè¯„åˆ†ï¼‰ ----
if AutoTokenizer is None:
    raise RuntimeError("ç¼ºå°‘ transformersï¼Œè¯·å®‰è£…ï¼špip install transformers accelerate")

print("ğŸ¤– ä½¿ç”¨åŠç²¾åº¦ + åˆ†æ‰¹åŠ è½½ + eval æ¨¡å¼åŠ è½½ Baichuan-7B ...")

# æ¸…ç†æ˜¾å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# tokenizer
tokenizer = AutoTokenizer.from_pretrained(BAICHUAN_PATH, trust_remote_code=True)

# é‡‡ç”¨åŠç²¾åº¦ + device_map è‡ªåŠ¨åˆ†é… + offload_folderï¼ˆè‹¥æœ‰ï¼‰
load_kwargs = dict(
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True,
)
# offload_folder è‹¥ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
offload_dir = Path("./offload")
try:
    offload_dir.mkdir(parents=True, exist_ok=True)
    load_kwargs["offload_folder"] = str(offload_dir)
except Exception:
    pass

# è½½å…¥æ¨¡å‹ï¼Œå¯èƒ½ä¼šèŠ±ä¸€ç‚¹æ—¶é—´
model = AutoModelForCausalLM.from_pretrained(BAICHUAN_PATH, **load_kwargs)
model.eval()

# è¾…åŠ©ï¼šå®‰å…¨è§£æ JSONï¼ˆä»æ¨¡å‹è¿”å›ä¸­æå–ç¬¬ä¸€ä¸ª {...}ï¼‰
def safe_json_parse(s: str):
    s = s.strip()
    try:
        return json.loads(s)
    except json.JSONDecodeError:
        m = re.search(r"\{.*?\}", s, re.S)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                # è¿›ä¸€æ­¥å°è¯•å»æ‰å¤šè¡Œæ³¨é‡Šæˆ–æ¨¡å‹å¸¸è¾“å‡ºçš„å‰ç¼€/åç¼€
                # ä¿åº•è¿”å›åŸæ–‡
                return {"_parse_error": True, "raw": s}
        return {"_parse_error": True, "raw": s}

# llm ç”Ÿæˆï¼Œå¸¦æ˜¾å­˜å‹å¥½ç­–ç•¥
def llm_generate(prompt, max_new_tokens=256):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
    # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹çš„é¦–ä¸ªå‚æ•°æ‰€åœ¨è®¾å¤‡ï¼ˆaccelerate çš„ device_map ç®¡ç†ï¼‰
    try:
        device = next(model.parameters()).device
    except StopIteration:
        device = torch.device("cpu")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    try:
        with torch.inference_mode():
            out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    except RuntimeError as e:
        # OOM é™çº§ç­–ç•¥ï¼šæ¸…ç©ºç¼“å­˜å¹¶é‡è¯•ä¸€æ¬¡ï¼ˆè‹¥ä»å¤±è´¥ï¼Œä¼šæŠ›å‡ºï¼‰
        print("Warning: ç”Ÿæˆæ—¶å‡ºç° RuntimeErrorï¼ˆå¯èƒ½OOMï¼‰ï¼Œå°è¯•å›æ”¶æ˜¾å­˜åé‡è¯•...", e)
        torch.cuda.empty_cache()
        with torch.inference_mode():
            out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    # è§£ç 
    text = tokenizer.decode(out[0], skip_special_tokens=True)
    if text.startswith(prompt):
        text = text[len(prompt):].strip()
    # å°è¯•å›æ”¶æ˜¾å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    return text

# llm-based scoringï¼ˆæ›´å¥å£®ï¼‰
def llm_score_semantic(gold, pred, timeout_sec=30):
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¯„ä¼°åŠ©æ‰‹ã€‚ä»»åŠ¡ï¼šæ¯”è¾ƒâ€œå‚è€ƒç­”æ¡ˆï¼ˆgoldï¼‰â€ä¸â€œæ¨¡å‹ç­”æ¡ˆï¼ˆpredï¼‰â€åœ¨è¯­ä¹‰å±‚é¢æ˜¯å¦ä¸€è‡´ï¼Œå¹¶ä»ä¸‰ä¸ªç»´åº¦æ‰“åˆ†ï¼ˆ0åˆ°1ï¼Œä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼š
å‚è€ƒç­”æ¡ˆ:
{gold}

æ¨¡å‹ç­”æ¡ˆ:
{pred}

è¯·æŒ‰ JSON è¾“å‡ºï¼Œä¾‹å¦‚ï¼š
{{"semantic_score":0.85,"coverage_score":0.70,"expression_score":0.90,"overall":0.82}}

ä¸è¦è¾“å‡ºå…¶ä»–é JSON å†…å®¹ã€‚"""
    try:
        out = llm_generate(prompt, max_new_tokens=200)
        j = safe_json_parse(out)
        # æ ‡å‡†åŒ–å­—æ®µ
        for k in ["semantic_score", "coverage_score", "expression_score", "overall"]:
            if k in j:
                try:
                    v = float(j[k])
                except Exception:
                    v = 0.0
                j[k] = max(0.0, min(1.0, v))
            else:
                j[k] = 0.0
        return j
    except Exception as e:
        print("LLM-based scoring å¤±è´¥ï¼š", e)
        return {"semantic_score":0.0, "coverage_score":0.0, "expression_score":0.0, "overall":0.0}

# ---- æ ¸å¿ƒ loopï¼šå¯¹æ¯ä¸ª embedding æ¨¡å‹åˆ†åˆ«è¯„ä¼° ----
summary_rows = []
all_details = {}
for embed_dir in EMBED_DIRS:
    name = Path(embed_dir).name
    print(f"\n=== å¤„ç† embedding æ¨¡å‹ï¼š{name} (dir={embed_dir}) ===")
    
    # âœ… å›ºå®šä½¿ç”¨æœ¬åœ° SentenceTransformer ç‰ˆæœ¬
    embedder_path = "/root/Agent/e5-base-v2_sbert"
    embedder = SentenceTransformer(embedder_path, device=DEVICE)


    print("ğŸ“¦ è®¡ç®—æ–‡æ¡£å‘é‡å¹¶å»ºç«‹ FAISS ç´¢å¼• ...")
    doc_embeddings = embedder.encode(doc_texts, batch_size=EMBED_BATCH, convert_to_tensor=False, show_progress_bar=True)
    doc_embeddings = np.array(doc_embeddings).astype("float32")

    if faiss is not None:
        dim = doc_embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)
        faiss.normalize_L2(doc_embeddings)
        index.add(doc_embeddings)
    else:
        index = None

    results = []
    total_hit = 0
    semantic_scores = []
    times = []

    for item in tqdm(tests, desc="questions"):
        q = str(item["question"]) if item.get("question") is not None else ""
        gold = str(item.get("gold_answer", ""))
        # 1) search
        q_emb = embedder.encode([q], convert_to_tensor=False)[0].astype("float32")
        if faiss is not None:
            q_emb_np = q_emb.reshape(1, -1)
            faiss.normalize_L2(q_emb_np)
            D, I = index.search(q_emb_np, TOP_K)
            idxs = I[0].tolist()
        else:
            sims = (doc_embeddings @ q_emb) / (np.linalg.norm(doc_embeddings, axis=1) * (np.linalg.norm(q_emb)+1e-12))
            idxs = sims.argsort()[::-1][:TOP_K].tolist()

        retrieved_texts = [doc_texts[i] for i in idxs if i < len(doc_texts)]
        retrieved_ids = [doc_ids[i] for i in idxs if i < len(doc_texts)]

        # 2) semantic hit
        gold_emb = embedder.encode([gold], convert_to_tensor=True)
        retrieved_embs = embedder.encode(retrieved_texts, convert_to_tensor=True)
        cos_scores = util.cos_sim(gold_emb, retrieved_embs)[0].cpu().numpy() if len(retrieved_texts) > 0 else np.array([0.0])
        hit_flag = int(cos_scores.max() > 0.80) if len(cos_scores) > 0 else 0
        total_hit += hit_flag

        # 3) generate
        prompt_docs = "\n\n".join([f"[{i}] {t}" for i, t in enumerate(retrieved_texts, start=1)])
        prompt = f"æ ¹æ®ä¸‹é¢æ£€ç´¢åˆ°çš„è¯æ®ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¦æ±‚ç®€æ´ä¸”å‡†ç¡®ï¼ˆè‹¥ç¼ºä¿¡æ¯è¯·æ³¨æ˜æ— è¶³å¤Ÿä¿¡æ¯ï¼‰ï¼š\n\nè¯æ®ï¼š\n{prompt_docs}\n\né—®é¢˜ï¼š\n{q}\n\nè¯·ç»™å‡ºä¸­æ–‡å›ç­”ï¼š"
        t0 = time.time()
        generated = llm_generate(prompt, max_new_tokens=256)
        t1 = time.time()
        gen_time = t1 - t0
        times.append(gen_time)

        # 4) compute metrics
        bleu = 0.0
        rouge_l = 0.0
        em = 0
        if sacrebleu is not None:
            try:
                bleu = sacrebleu.sentence_bleu(generated, [gold]).score/100.0
            except Exception:
                bleu = 0.0
        if rouge_scorer is not None:
            try:
                scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=False)
                r = scorer.score(gold, generated)
                rouge_l = r['rougeL'].fmeasure
            except Exception:
                rouge_l = 0.0
        def normalize_whitespace(s): return "".join(str(s).split())
        em = 1 if normalize_whitespace(generated) == normalize_whitespace(gold) else 0

        # 5) llm self-eval
        self_eval = llm_score_semantic(gold, generated)
        semantic_scores.append(self_eval.get("overall", 0.0))

        results.append({
            "question": q,
            "gold_answer": gold,
            "generated_answer": generated,
            "retrieved_ids": retrieved_ids,
            "retrieved_texts": retrieved_texts,
            "hit_flag": hit_flag,
            "time_sec": gen_time,
            "bleu": bleu,
            "rouge_l": rouge_l,
            "em": em,
            "semantic_overall": self_eval.get("overall", 0.0),
            "semantic_detail": json.dumps(self_eval, ensure_ascii=False)
        })

    # aggregate
    n = len(results)
    mean_bleu = float(np.mean([r["bleu"] for r in results])) if n>0 else 0.0
    mean_rouge = float(np.mean([r["rouge_l"] for r in results])) if n>0 else 0.0
    mean_em = float(np.mean([r["em"] for r in results])) if n>0 else 0.0
    mean_time = float(np.mean([r["time_sec"] for r in results])) if n>0 else 0.0
    hit_rate = total_hit / n if n>0 else 0.0
    mean_semantic = float(np.mean(semantic_scores)) if len(semantic_scores)>0 else 0.0

    out_csv = Path(OUTPUT_DIR) / f"results_{name}.csv"
    pd.DataFrame(results).to_csv(out_csv, index=False)
    print(f"âœ… å·²ä¿å­˜æ¯é¢˜ç»“æœï¼š{out_csv} (n={n})")

    summary_rows.append({
        "embed_model": name,
        "mean_bleu": mean_bleu,
        "mean_rouge_l": mean_rouge,
        "mean_em": mean_em,
        "mean_time_sec": mean_time,
        "retrieval_hit_rate": hit_rate,
        "semantic_overall": mean_semantic,
        "n": n
    })
    all_details[name] = results

# === æ±‡æ€»åˆ° Excel ===
df_summary = pd.DataFrame(summary_rows)
excel_path = Path(OUTPUT_DIR) / "rag_all_results.xlsx"
with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
    df_summary.to_excel(writer, sheet_name="summary", index=False)
    for k, v in all_details.items():
        pd.DataFrame(v).to_excel(writer, sheet_name=k[:31], index=False)

print(f"\nğŸ“Š æ±‡æ€»å·²ä¿å­˜åˆ° Excel: {excel_path}")

# === ç”Ÿæˆè®ºæ–‡å›¾è¡¨ ===
if plt is None:
    print("matplotlib æœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
else:
    print("ğŸ“ˆ ç”Ÿæˆè®ºæ–‡å›¾è¡¨ ...")
    # try seaborn style, otherwise fallback
    try:
        import seaborn as sns
        sns.set_style("whitegrid")
    except Exception:
        try:
            plt.style.use('seaborn-v0_8-whitegrid')
        except Exception:
            plt.style.use('default')

    x = df_summary['embed_model']
    fig, ax = plt.subplots(figsize=(8,5))
    ax.bar(x, df_summary['mean_bleu'], label="BLEU")
    ax.bar(x, df_summary['mean_rouge_l'], label="ROUGE-L", alpha=0.9, bottom=0)
    ax.set_title("Embedding model comparison (BLEU / ROUGE-L)")
    ax.set_ylabel("score")
    ax.legend()
    fig.savefig(charts_dir / "bleu_rouge_comparison.png", dpi=300, bbox_inches="tight")

    fig, ax = plt.subplots(figsize=(8,5))
    ax.bar(x, df_summary['retrieval_hit_rate'])
    ax.set_title("Top-K retrieval hit rate (gold included in retrieved text)")
    ax.set_ylabel("hit rate")
    fig.savefig(charts_dir / "retrieval_hit_rate.png", dpi=300, bbox_inches="tight")

    fig, ax = plt.subplots(figsize=(8,5))
    ax.bar(x, df_summary['semantic_overall'])
    ax.set_title("Embedding model comparison (LLM-based semantic overall score)")
    ax.set_ylabel("semantic overall (0-1)")
    fig.savefig(charts_dir / "semantic_score.png", dpi=300, bbox_inches="tight")

    fig, ax = plt.subplots(figsize=(8,5))
    ax.scatter(df_summary['mean_time_sec'], df_summary['mean_bleu'])
    for i, txt in enumerate(x):
        ax.annotate(txt, (df_summary['mean_time_sec'].iloc[i], df_summary['mean_bleu'].iloc[i]))
    ax.set_xlabel("avg gen time (s)")
    ax.set_ylabel("avg BLEU")
    ax.set_title("Time vs BLEU (per embedding model)")
    fig.savefig(charts_dir / "time_vs_bleu.png", dpi=300, bbox_inches="tight")

    print(f"âœ… å›¾è¡¨ä¿å­˜åœ¨ï¼š{charts_dir}")

print("ğŸ‰ æµç¨‹å®Œæˆã€‚è¯·æŸ¥çœ‹ results/ ä¸‹çš„ Excel/CSV ä¸ charts_paper/ å›¾ç‰‡ï¼Œç”¨äºè®ºæ–‡ç»˜å›¾å’Œè¡¨æ ¼ã€‚")
