# -*- coding: utf-8 -*-
"""
LLM-based ç”Ÿæˆè´¨é‡è¯„ä¼°æ¨¡å—
ä½œè€…: GPT-5
è¯´æ˜: ä½¿ç”¨ Baichuan-7B æ¨¡å‹å¯¹ RAG ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡è¿›è¡Œ LLM æ‰“åˆ†ã€‚
è¯„åˆ†ç»´åº¦: æ­£ç¡®æ€§ / å®Œæ•´æ€§ / è¡¨è¾¾æ¸…æ™°åº¦ (æ€»åˆ† 1-10)
"""

import os
import json
import pandas as pd
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt

# ==============================
# å‚æ•°é…ç½®
# ==============================
BAICHUAN_PATH = "/root/Agent/baichuan-7b"  # ä½ çš„ Baichuan æ¨¡å‹ç›®å½•
RESULTS_DIR = "./results"
INPUT_CSV = os.path.join(RESULTS_DIR, "results_e5-base-v2.csv")
OUTPUT_CSV = os.path.join(RESULTS_DIR, "results_with_llm_score.csv")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ==============================
# åŠ è½½æ¨¡å‹ï¼ˆåŠç²¾åº¦ + eval æ¨¡å¼ï¼‰
# ==============================
print("ğŸ¤– åŠ è½½ Baichuan-7B æ¨¡å‹ (åŠç²¾åº¦ + eval æ¨¡å¼)...")
tokenizer = AutoTokenizer.from_pretrained(BAICHUAN_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    BAICHUAN_PATH,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)
model.eval()

# ==============================
# å®šä¹‰æ‰“åˆ†å‡½æ•°
# ==============================
def llm_score_response(question, evidence, answer):
    """
    ä½¿ç”¨ Baichuan ç”Ÿæˆæ‰“åˆ†ï¼ˆè¾“å‡ºä¸€ä¸ª 1~10 çš„æ•´æ•°ï¼‰
    """
    prompt = f"""
è¯·ä½ æ‰®æ¼”ä¸€ä¸ªè¯„ä¼°ä¸“å®¶ï¼Œé’ˆå¯¹ä»¥ä¸‹é—®ç­”ç»“æœæ‰“åˆ†ã€‚
ã€è¯„åˆ†æ ‡å‡†ã€‘
1. æ­£ç¡®æ€§ï¼ˆæ˜¯å¦ä¸äº‹å®ä¸€è‡´ï¼‰ï¼›
2. å®Œæ•´æ€§ï¼ˆæ˜¯å¦åŒ…å«å¿…è¦è¦ç‚¹ï¼‰ï¼›
3. è¡¨è¾¾æ¸…æ™°åº¦ï¼ˆæ˜¯å¦è¯­å¥é€šé¡ºæ˜“æ‡‚ï¼‰ï¼›
æ€»åˆ† 1~10 åˆ†ï¼Œè¾“å‡ºä¸€ä¸ªæ•´æ•°åˆ†æ•°ï¼Œä¸è¦è§£é‡Šã€‚

ã€é—®é¢˜ã€‘
{question}

ã€æ£€ç´¢åˆ°çš„è¯æ®ã€‘
{evidence}

ã€æ¨¡å‹å›ç­”ã€‘
{answer}

è¯·ç›´æ¥è¾“å‡ºä¸€ä¸ªæ•´æ•°åˆ†æ•°ï¼š
"""
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(next(model.parameters()).device)
        outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)
        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æå–åˆ†æ•°ï¼ˆç®€å•æ­£åˆ™ï¼‰
        import re
        match = re.search(r"(\d{1,2})", text)
        if match:
            score = int(match.group(1))
            score = max(1, min(score, 10))  # é™åˆ¶èŒƒå›´
        else:
            score = None
    except Exception as e:
        print(f"âš ï¸ LLM æ‰“åˆ†å¤±è´¥: {e}")
        score = None
    return score


# ==============================
# è¯»å–å·²æœ‰ç»“æœ
# ==============================
print(f"ğŸ“‚ è¯»å–ç»“æœæ–‡ä»¶: {INPUT_CSV}")
df = pd.read_csv(INPUT_CSV)
# ---- è‡ªåŠ¨åˆ—åæ˜ å°„ ----
col_map = {
    "retrieved_docs": "retrieved_texts",
    "generated_text": "generated_answer"
}

for old, new in col_map.items():
    if old not in df.columns and new in df.columns:
        df[old] = df[new]

# ---- æ£€æŸ¥åˆ—æ˜¯å¦é½å…¨ ----
if not all(col in df.columns for col in ["question", "retrieved_docs", "generated_text"]):
    raise ValueError("CSV æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: question / retrieved_docs / generated_text")


scores = []

# ==============================
# æ‰§è¡Œè¯„åˆ†
# ==============================
for i, row in tqdm(df.iterrows(), total=len(df), desc="LLM-based ç”Ÿæˆè´¨é‡è¯„ä¼°ä¸­"):
    q = str(row["question"])
    evi = str(row["retrieved_docs"])[:800]  # é¿å…è¶…é•¿
    ans = str(row["generated_text"])
    score = llm_score_response(q, evi, ans)
    scores.append(score)

df["LLM_Score"] = scores
df.to_csv(OUTPUT_CSV, index=False)
print(f"âœ… å·²ä¿å­˜ LLM æ‰“åˆ†ç»“æœ: {OUTPUT_CSV}")

# ==============================
# æ±‡æ€»ç»Ÿè®¡ä¸å›¾è¡¨è¾“å‡º
# ==============================
mean_score = df["LLM_Score"].dropna().mean()
print(f"ğŸ“Š å¹³å‡ LLM è¯„åˆ†: {mean_score:.2f}")

# ç”Ÿæˆè®ºæ–‡ç”¨å›¾
plt.figure(figsize=(7, 5))
plt.hist(df["LLM_Score"].dropna(), bins=10, color="#5DADE2", edgecolor="black")
plt.title("LLM-based ç”Ÿæˆè´¨é‡å¾—åˆ†åˆ†å¸ƒ", fontsize=14)
plt.xlabel("è¯„åˆ† (1~10)")
plt.ylabel("æ ·æœ¬æ•°é‡")
plt.grid(alpha=0.3)
chart_path = os.path.join(RESULTS_DIR, "charts_paper/llm_score_distribution.png")
os.makedirs(os.path.dirname(chart_path), exist_ok=True)
plt.savefig(chart_path, dpi=300, bbox_inches="tight")
plt.close()
print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜: {chart_path}")

print("ğŸ¯ LLM-based è´¨é‡è¯„ä¼°å®Œæˆï¼Œå¯ç›´æ¥ç”¨äºè®ºæ–‡ç»“æœç« èŠ‚ã€‚")
