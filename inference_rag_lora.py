# inference_rag_lora.py
# ğŸ¯ åŸºäº LoRA å¾®è°ƒæƒé‡ + æ£€ç´¢å¢å¼ºçš„å¤šè½®é—®ç­”æ¨ç†æ¨¡æ¿

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer
import faiss
import json

# ===============================
# ğŸ”¹ é…ç½®
# ===============================
MODEL_PATH = "/root/Agent/out_lora_qlo_v3"  # è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹
BASE_MODEL = "baichuan-7b"                 # ç”¨äº tokenizer å’Œ base model
DEVICE = "cuda"
MAX_INPUT_LENGTH = 512
MAX_NEW_TOKENS = 256

# æ£€ç´¢æ•°æ®
DOCS_PATH = "./knowledge_base_text.jsonl"  # æ¯è¡Œ: {"text": "..."}
EMBED_MODEL = "all-MiniLM-L6-v2"      # å°å‹ embedding æ¨¡å‹
TOP_K = 3                             # æ£€ç´¢ top-k

# ===============================
# ğŸ”¹ åŠ è½½æ¨¡å‹
# ===============================
print("ğŸ”¹ Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL, 
    use_fast=True, 
    trust_remote_code=True  # âœ… å…è®¸æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    trust_remote_code=True  # âœ… å…è®¸æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
)
model.eval()


# ===============================
# ğŸ”¹ åŠ è½½çŸ¥è¯†åº“å¹¶æ„å»ºå‘é‡ç´¢å¼•
# ===============================
print("ğŸ”¹ Loading knowledge base and building FAISS index...")
texts = []
with open(DOCS_PATH, "r", encoding="utf-8") as f:
    for line in f:
        item = json.loads(line)
        texts.append(item["text"])

embed_model = SentenceTransformer(EMBED_MODEL, device=DEVICE)
embeddings = embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)

dim = embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(embeddings)
print(f"âœ… Knowledge base loaded, {len(texts)} documents indexed.")

# ===============================
# ğŸ”¹ å¤šè½®é—®ç­”å‡½æ•°
# ===============================
conversation_history = []

def retrieve_context(query, top_k=TOP_K):
    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    scores, ids = index.search(q_emb, top_k)
    context = "\n".join([texts[i] for i in ids[0]])
    return context

def chat(query, use_rag=True):
    global conversation_history
    conversation_history.append({"role": "user", "content": query})

    # æ„é€  prompt
    prompt_text = ""
    for turn in conversation_history:
        role = "ç”¨æˆ·" if turn["role"] == "user" else "åŠ©æ‰‹"
        prompt_text += f"{role}: {turn['content']}\n"
    
    # æ£€ç´¢å¢å¼º
    if use_rag:
        context = retrieve_context(query)
        prompt_text = f"ä»¥ä¸‹æ˜¯ç›¸å…³çŸ¥è¯†:\n{context}\n\n{prompt_text}"

    # Tokenize
    inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True,
                       padding=True, max_length=MAX_INPUT_LENGTH).to(DEVICE)

    # ç”Ÿæˆ
    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # å»æ‰ prompt éƒ¨åˆ†ï¼Œåªä¿ç•™æœ€æ–°å›ç­”
    response = response.replace(prompt_text, "").strip()
    conversation_history.append({"role": "assistant", "content": response})
    return response

# ===============================
# ğŸ”¹ æµ‹è¯•å¤šè½®é—®ç­”
# ===============================
if __name__ == "__main__":
    print("\nâœ¨ RAG + LoRA Chat Bot Ready! è¾“å…¥ 'exit' ç»“æŸå¯¹è¯.\n")
    while True:
        query = input("ç”¨æˆ·: ")
        if query.lower() in ["exit", "quit"]:
            break
        answer = chat(query)
        print("åŠ©æ‰‹:", answer)
