# prompt_opt_ablation.py
# Prompt ä¼˜åŒ– Ablation: åŸ vs ä¼˜åŒ– promptï¼Œæå‡ BLEU/EM (n=19 queries)

import os
import time
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import matplotlib.pyplot as plt
import gc
import json  # LLM Score JSON è§£æ

# ========== è·¯å¾„é…ç½® ==========
E5_PATH = "/root/Agent/e5-base-v2_sbert"
BAICHUAN_PATH = "/root/Agent/baichuan-7b"
DOCS_FILE = "/root/Agent/documents.csv"
QUESTIONS_FILE = "/root/Agent/rag_test_questions.csv"
SAVE_DIR = "./results/prompt_opt"
os.makedirs(SAVE_DIR, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")

# ========== åŠ è½½æ¨¡å‹ ==========
print("ğŸ¤– åŠ è½½ E5 embedding ...")
retriever = SentenceTransformer(E5_PATH, device=device)

print("ğŸ§  åŠ è½½ Baichuan-7B ...")
tokenizer = AutoTokenizer.from_pretrained(BAICHUAN_PATH, local_files_only=True, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

llm_model = AutoModelForCausalLM.from_pretrained(
    BAICHUAN_PATH, torch_dtype=torch.float16, device_map="auto" if device == "cuda" else None,
    local_files_only=True, trust_remote_code=True
)
if device == "cuda":
    llm_model = llm_model.to(device)
llm_model.eval()
if device == "cuda":
    torch.cuda.empty_cache()
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# ========== åŠ è½½æ•°æ® ==========
docs_df = pd.read_csv(DOCS_FILE)
documents = docs_df["text"].dropna().tolist()
doc_embeds = retriever.encode(documents, convert_to_tensor=True, normalize_embeddings=True)
print(f"ğŸ“š æ–‡æ¡£åº“: {len(documents)} æ¡")

test_df = pd.read_csv(QUESTIONS_FILE)
test_data = [{"query": row["question"], "gold": row["gold_answer"]} for _, row in test_df.iterrows() if pd.notna(row["question"])]
print(f"â“ æµ‹è¯•é›†: {len(test_data)} æ¡")

# ========== æŒ‡æ ‡å‡½æ•° ==========
def evaluate_metrics(pred, gold, gen_time):
    smooth = SmoothingFunction().method1
    bleu = sentence_bleu([gold.split()], pred.split(), smoothing_function=smooth)
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    rouge = scorer.score(gold, pred)["rougeL"].fmeasure
    em = 1.0 if pred.strip().lower() == gold.strip().lower() else 0.0
    return bleu, rouge, em, gen_time

def llm_evaluate(generated, gold):
    eval_prompt = f"Rate the generation vs gold answer on 0-1 scale: semantic (consistency), coverage (evidence use), expression (fluency). Output JSON: {{\"semantic\":0.85, \"coverage\":0.7, \"expression\":0.9, \"overall\":0.82}}"
    inputs = tokenizer(eval_prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = llm_model.generate(**inputs, max_new_tokens=50, temperature=0.1, do_sample=False)
    score_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    try:
        scores = json.loads(score_text.split("JSON:")[-1].strip())  # è§£æ JSON
        overall = scores.get("overall", 0.5)
    except:
        overall = 0.5  # fallback
    return overall

# ========== ç”Ÿæˆå‡½æ•° (é€šç”¨) ==========
def generate(prompt, max_new_tokens=100):
    start_time = time.time()
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    with torch.no_grad():
        outputs = llm_model.generate(
            **inputs, max_new_tokens=max_new_tokens, temperature=0.7, do_sample=True, top_p=0.9,
            pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id
        )
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    gen_time = time.time() - start_time
    del inputs, outputs
    if device == "cuda":
        torch.cuda.empty_cache()
        gc.collect()
    return generated, gen_time

# ========== RAG ç”Ÿæˆ (æŒ‡å®š prompt å˜ä½“) ==========
def rag_generate(query, top_k=5, prompt_template="original"):
    q_emb = retriever.encode(query, convert_to_tensor=True, normalize_embeddings=True)
    scores = (q_emb @ doc_embeds.T).squeeze(0)
    top_idx = torch.topk(scores, top_k).indices.tolist()
    retrieved_text = "\n".join([documents[i] for i in top_idx])
    
    if prompt_template == "original":
        prompt = f"ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„è¯æ®ï¼š\n{retrieved_text}\n\nåŸºäºä»¥ä¸Šå†…å®¹ï¼Œç®€çŸ­å›ç­”ï¼š{query}"
    elif prompt_template == "optimized":
        prompt = f"åŸºäºè¯æ®ï¼Œè¾“å‡ºç®€æ´ç­”æ¡ˆï¼Œä¸è¦é‡å¤é—®é¢˜æˆ–è¯æ®ï¼Œåªè¾“å‡ºæ ¸å¿ƒäº‹å®ã€‚é—®é¢˜ï¼š{query} è¯æ®ï¼š{retrieved_text} ç­”æ¡ˆï¼š"
    else:
        raise ValueError("Prompt template must be 'original' or 'optimized'")
    
    return generate(prompt)

# ========== å®éªŒæ‰§è¡Œ ==========
results = []
print("ğŸš€ Prompt Ablation: Original vs Optimized (n=19) ...")
for item in tqdm(test_data):
    q, gold = item["query"], item["gold"]
    
    # Original Prompt
    rag_orig_pred, rag_orig_time = rag_generate(q, prompt_template="original")
    bleu_orig, rouge_orig, em_orig, _ = evaluate_metrics(rag_orig_pred, gold, rag_orig_time)
    llm_orig = llm_evaluate(rag_orig_pred, gold)
    results.append([q, "RAG_Original", rag_orig_pred, bleu_orig, rouge_orig, em_orig, rag_orig_time, llm_orig])
    
    # Optimized Prompt
    rag_opt_pred, rag_opt_time = rag_generate(q, prompt_template="optimized")
    bleu_opt, rouge_opt, em_opt, _ = evaluate_metrics(rag_opt_pred, gold, rag_opt_time)
    llm_opt = llm_evaluate(rag_opt_pred, gold)
    results.append([q, "RAG_Optimized", rag_opt_pred, bleu_opt, rouge_opt, em_opt, rag_opt_time, llm_opt])

# ========== æ±‡æ€» ==========
df = pd.DataFrame(results, columns=["query", "mode", "prediction", "BLEU", "ROUGE-L", "EM", "Time (s)", "LLM Score"])
csv_path = os.path.join(SAVE_DIR, "prompt_opt_results.csv")
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

orig_avg = df[df["mode"] == "RAG_Original"][["BLEU", "ROUGE-L", "EM", "Time (s)", "LLM Score"]].mean()
opt_avg = df[df["mode"] == "RAG_Optimized"][["BLEU", "ROUGE-L", "EM", "Time (s)", "LLM Score"]].mean()

print("\nğŸ“Š å¹³å‡ç»“æœï¼š")
print(f"RAG Original:\n{orig_avg}")
print(f"RAG Optimized:\n{opt_avg}")
print(f"æå‡%:\n{((opt_avg - orig_avg) / orig_avg * 100).round(1)}")

# ========== ç»˜å›¾ ==========
plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# è´¨é‡æŒ‡æ ‡
labels = ["BLEU", "ROUGE-L", "EM"]
orig_scores = orig_avg[:3].values
opt_scores = opt_avg[:3].values
x = np.arange(len(labels))
width = 0.35
ax1.bar(x - width/2, orig_scores, width, label="Original", color="#4472C4", alpha=0.7)
ax1.bar(x + width/2, opt_scores, width, label="Optimized", color="#ED7D31", alpha=0.7)
ax1.set_ylabel("Score")
ax1.set_title("è´¨é‡æŒ‡æ ‡å¯¹æ¯” (Pre/Post Opt)")
ax1.set_xticks(x)
ax1.set_xticklabels(labels)
ax1.set_ylim(0, 0.3)
ax1.legend()

# LLM Score & Time
ax2.bar(["LLM Score", "Time (s)"], [orig_avg["LLM Score"], orig_avg["Time (s)"]], color="#4472C4", alpha=0.7, label="Original")
ax2.bar(["LLM Score", "Time (s)"], [opt_avg["LLM Score"], opt_avg["Time (s)"]], color="#ED7D31", alpha=0.7, label="Optimized", bottom=[orig_avg["LLM Score"], orig_avg["Time (s)"]])
ax2.set_ylabel("Value")
ax2.set_title("LLM Score & Time å¯¹æ¯”")
ax2.legend()

plt.tight_layout()
chart_path = os.path.join(SAVE_DIR, "prompt_opt_chart.png")
plt.savefig(chart_path, dpi=300, bbox_inches="tight")
plt.close()

print(f"âœ… Ablation å®Œæˆï¼CSV: {csv_path}, å›¾: {chart_path}")