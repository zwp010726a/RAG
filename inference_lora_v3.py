# inference_lora_v3.py
import os
import time
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# --------------------
# é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
# --------------------
BASE_MODEL = "/root/Agent/baichuan-7b"            # æœ¬åœ°åŸºç¡€æ¨¡å‹è·¯å¾„
LORA_WEIGHTS = "/root/Agent/out_lora_qlo_v3"     # è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆå« adapter_model.safetensors/binï¼‰
MAX_NEW_TOKENS = 128
TEMPERATURE = 0.1
TOP_P = 0.95
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# --------------------

def load_tokenizer_and_base(model_path):
    print(f"ğŸ”¹ Loading tokenizer from {model_path} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True, local_files_only=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    return tokenizer

def load_base_model_4bit(model_path):
    print(f"ğŸ”¹ Loading base model (4-bit) from {model_path} ...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True
    )
    return model

def load_peft_model(base_model, lora_path):
    # è‡ªåŠ¨è¯†åˆ« safetensors / bin
    if os.path.exists(os.path.join(lora_path, "adapter_model.safetensors")):
        print("âœ… Found adapter_model.safetensors, will load adapter from safetensors.")
    elif os.path.exists(os.path.join(lora_path, "adapter_model.bin")):
        print("âœ… Found adapter_model.bin, will load adapter from bin.")
    else:
        print("âš ï¸ No adapter_model.* found in LORA path; attempting to load whatever is present.")
    print(f"ğŸ”¹ Loading LoRA adapter from: {lora_path} ...")
    # æ³¨æ„ device_map="auto" è®© PEFT å°è¯•æŠŠ adapter æ”¾åˆ° GPU
    pefted = PeftModel.from_pretrained(base_model, lora_path, device_map="auto", torch_dtype=torch.float16, local_files_only=True)
    return pefted

def print_param_info(model):
    try:
        s = model.print_trainable_parameters()
        # model.print_trainable_parameters prints to stdout; return None. We still call it to show info.
    except Exception:
        pass

def generate_answer(model, tokenizer, question, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, top_p=TOP_P):
    # ä¸è®­ç»ƒæ ¼å¼ä¿æŒä¸€è‡´çš„ prompt å‰ç¼€
    prompt = f"è¯·æ ¹æ®è¯æ®å›ç­”ï¼š{question}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(model.device)
    t0 = time.time()
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    t1 = time.time()
    answer = tokenizer.decode(out[0], skip_special_tokens=True)
    # å»æ‰ prompt å‰ç¼€ï¼ˆè‹¥æ¨¡å‹ç›´æ¥è¾“å‡ºåŒ…å« promptï¼‰
    if answer.startswith(prompt):
        answer = answer[len(prompt):].strip()
    used_tokens = out.shape[1]
    elapsed = t1 - t0
    # GPU mem info
    gpu_mem = None
    if torch.cuda.is_available():
        gpu_mem = {
            "allocated_MB": torch.cuda.memory_allocated(0) / 1024**2,
            "reserved_MB": torch.cuda.memory_reserved(0) / 1024**2
        }
    return answer, elapsed, used_tokens, gpu_mem

def main():
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0) if DEVICE=='cuda' else 'CPU'}")
    tokenizer = load_tokenizer_and_base(BASE_MODEL)
    base = load_base_model_4bit(BASE_MODEL)
    model = load_peft_model(base, LORA_WEIGHTS)
    print_param_info(model)
    model.eval()

    print("\nğŸš€ LoRA æ¨ç†å·²å°±ç»ªï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰\n" + "="*60)
    try:
        while True:
            question = input("ğŸ§  è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š\n> ").strip()
            if not question:
                continue
            if question.lower() in ("exit", "quit"):
                break
            answer, elapsed, used_tokens, gpu_mem = generate_answer(model, tokenizer, question)
            result = {
                "question": question,
                "answer": answer,
                "time_s": round(elapsed, 3),
                "used_tokens": int(used_tokens),
                "gpu_mem": {k: round(v,1) for k,v in gpu_mem.items()} if gpu_mem is not None else None
            }
            print("\nğŸ¤– æ¨¡å‹å›ç­”ï¼š")
            print(json.dumps(result, ensure_ascii=False, indent=4))
            print("\n" + "="*60 + "\n")
    except KeyboardInterrupt:
        print("\né€€å‡ºã€‚")
    finally:
        # æ¸…ç†
        try:
            del model
            del base
            torch.cuda.empty_cache()
        except Exception:
            pass

if __name__ == "__main__":
    main()
