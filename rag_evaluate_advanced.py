# rag_evaluate.py (é«˜çº§ç‰ˆ + Top-K æ£€ç´¢å‘½ä¸­ç»Ÿè®¡)
import os
import time
import json
import csv
from tqdm import tqdm
from collections import Counter
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# =====================
# é…ç½®
# =====================
EMBEDDING_MODEL_PATH = './all-MiniLM-L6-v2'
LLM_MODEL_PATH = './baichuan-7b'
TOP_K = 3
TEST_QUESTIONS_FILE = './test_questions.json'  # [{"question": "...", "answer": "..."}]
DOCUMENTS_FILE = './documents.json'             # [{"doc_id": 1, "text": "..."}]
OUTPUT_FILE = './rag_results.csv'

# =====================
# åŠ è½½æ¨¡å‹
# =====================
print("ğŸš€ åŠ è½½ SentenceTransformer å‘é‡æ¨¡å‹...")
embed_model = SentenceTransformer(EMBEDDING_MODEL_PATH)

print("ğŸ“¦ åŠ è½½æ–‡æ¡£å¹¶ç”Ÿæˆ FAISS ç´¢å¼•...")
with open(DOCUMENTS_FILE, 'r', encoding='utf-8') as f:
    docs = json.load(f)
doc_texts = [d['text'] for d in docs]

doc_embeddings = embed_model.encode(doc_texts, convert_to_numpy=True, batch_size=64)
dim = doc_embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
faiss.normalize_L2(doc_embeddings)
index.add(doc_embeddings)
print(f"ğŸ“Œ FAISS ç´¢å¼•ç”Ÿæˆå®Œæˆï¼Œæ–‡æ¡£æ•°é‡ï¼š{len(doc_texts)}")

print("ğŸ¤– åŠ è½½æœ¬åœ° LLM æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_PATH, trust_remote_code=True)
llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_PATH, trust_remote_code=True)
generator = pipeline('text-generation', model=llm_model, tokenizer=tokenizer, device=0 if os.getenv("CUDA_VISIBLE_DEVICES") else -1)
print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")

# =====================
# è¯»å–æµ‹è¯•é—®é¢˜
# =====================
with open(TEST_QUESTIONS_FILE, 'r', encoding='utf-8') as f:
    test_data = json.load(f)

# =====================
# RAG æŸ¥è¯¢å‡½æ•°
# =====================
def rag_query(question, top_k=TOP_K):
    start_time = time.time()

    # å‘é‡æ£€ç´¢
    q_vec = embed_model.encode([question], convert_to_numpy=True)
    faiss.normalize_L2(q_vec)
    D, I = index.search(q_vec, top_k)
    retrieved_docs = [doc_texts[i] for i in I[0]]

    # æ„é€  prompt
    prompt = "ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µï¼š\n"
    for idx, doc in enumerate(retrieved_docs, 1):
        prompt += f"[{idx}] {doc}\n"
    prompt += f"\nè¯·æ ¹æ®ä»¥ä¸Šå†…å®¹ç®€æ´å›ç­”é—®é¢˜ï¼š{question}"

    # ç”Ÿæˆå›ç­”
    output = generator(prompt, max_new_tokens=200, do_sample=True, temperature=0.3)[0]['generated_text']
    elapsed = time.time() - start_time
    return output.strip(), retrieved_docs, elapsed

# =====================
# æ‰¹é‡æµ‹è¯•
# =====================
results = []
times = []
lengths = []
top_k_hits = []

for item in tqdm(test_data, desc="Evaluating"):
    question = item['question']
    gold_answer = item.get('answer', '')
    generated, retrieved_docs, elapsed = rag_query(question)

    # Top-K æ£€ç´¢å‘½ä¸­ç»Ÿè®¡ï¼ˆç®€å•å­—ç¬¦ä¸²åŒ…å«åˆ¤æ–­ï¼‰
    hit_flags = [int(gold_answer in doc) for doc in retrieved_docs]
    top_k_hits.append(hit_flags)

    results.append({
        'question': question,
        'gold_answer': gold_answer,
        'generated_answer': generated,
        'retrieved_docs': "|".join(retrieved_docs),
        'time_sec': elapsed
    })
    times.append(elapsed)
    lengths.append(len(generated.split()))

# =====================
# ä¿å­˜ç»“æœ CSV
# =====================
with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=results[0].keys())
    writer.writeheader()
    for row in results:
        writer.writerow(row)
print(f"âœ… æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜è‡³ {OUTPUT_FILE}")

# =====================
# åŸºç¡€ç»Ÿè®¡åˆ†æ
# =====================
avg_time = np.mean(times)
avg_length = np.mean(lengths)
all_words = " ".join([r['generated_answer'] for r in results]).split()
unique_ratio = len(set(all_words)) / max(1, len(all_words))

# Top-K å‘½ä¸­ç‡ç»Ÿè®¡
top_k_hits = np.array(top_k_hits)
top1_rate = np.mean(top_k_hits[:,0])
top3_rate = np.mean(np.any(top_k_hits[:,:3], axis=1))
top5_rate = np.mean(np.any(top_k_hits[:,:5], axis=1)) if top_k_hits.shape[1]>=5 else None

print("\nğŸ“Š åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡:")
print(f"- å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f} ç§’")
print(f"- å¹³å‡å›ç­”é•¿åº¦: {avg_length:.1f} ä¸ªè¯")
print(f"- ç”Ÿæˆè¯å”¯ä¸€ç‡: {unique_ratio:.3f}")
print(f"- Top-1 æ£€ç´¢å‘½ä¸­ç‡: {top1_rate:.3f}")
print(f"- Top-3 æ£€ç´¢å‘½ä¸­ç‡: {top3_rate:.3f}")
if top5_rate is not None:
    print(f"- Top-5 æ£€ç´¢å‘½ä¸­ç‡: {top5_rate:.3f}")
