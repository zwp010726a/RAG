import torch
import os
import json
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset
from accelerate import Accelerator

# ==============================
# åŸºæœ¬é…ç½®ï¼ˆå…¨éƒ¨æœ¬åœ°ï¼‰
# ==============================
model_name = "/root/Agent/baichuan-7b"   # æœ¬åœ°æ¨¡å‹è·¯å¾„
train_file = "/root/Agent/train.jsonl"   # æœ¬åœ°è®­ç»ƒæ•°æ®
output_dir = "/root/Agent/out_lora_qlo_test"

# ==============================
# åˆå§‹åŒ– Accelerate
# ==============================
accelerator = Accelerator()
device = accelerator.device
print(f"ğŸš€ Using device: {device}")

# ==============================
# åŠ è½½åˆ†è¯å™¨ï¼ˆæœ¬åœ°ï¼Œä¸è”ç½‘ï¼‰
# ==============================
print("ğŸ”¹ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, local_files_only=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print("âœ… Tokenizer loaded from local path.")

# ==============================
# QLoRA é‡åŒ–é…ç½®
# ==============================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# ==============================
# åŠ è½½æ¨¡å‹ï¼ˆæœ¬åœ°ï¼Œä½¿ç”¨GPUï¼‰
# ==============================
print("ğŸ”¹ Loading base model (4-bit QLoRA, local only)...")
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True
)
print("âœ… Model loaded successfully from local path.")

# ==============================
# LoRA é…ç½®
# ==============================
target_modules = [
    "W_pack", "o_proj", "down_proj", "up_proj", "gate_proj"
]

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=target_modules,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
print("âœ… LoRA adapters applied.")

# ==============================
# åŠ è½½æœ¬åœ°æ•°æ®é›†
# ==============================
print("ğŸ”¹ Loading local dataset...")
with open(train_file, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]
print("Example sample:", data[0])

# å‡è®¾æ¯è¡Œæ•°æ®æ ¼å¼ä¸º {"text": "..."}ï¼Œè‹¥å­—æ®µä¸åŒè¯·ä¿®æ”¹
dataset = Dataset.from_list(data)

def tokenize_fn(examples):
    # å°† prompt + response æ‹¼æ¥ä¸ºå•æ¡æ–‡æœ¬è¾“å…¥
    texts = [
        f"ç”¨æˆ·ï¼š{p}\nåŠ©æ‰‹ï¼š{r}"
        for p, r in zip(examples['prompt'], examples['response'])
    ]
    return tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=256
    )


tokenized_dataset = dataset.map(tokenize_fn, batched=True)
split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

print(f"âœ… Loaded {len(train_dataset)} training samples and {len(eval_dataset)} eval samples.")

# ==============================
# æ•°æ®æ•´ç†å™¨
# ==============================
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ==============================
# è®­ç»ƒå‚æ•°
# ==============================
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=False,
    bf16=True,
    logging_steps=5,
    save_strategy="epoch",
    evaluation_strategy="no",
    report_to="none",
)

# ==============================
# Trainer + Accelerate
# ==============================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# å‡†å¤‡é˜¶æ®µ (Accelerate)
model, trainer = accelerator.prepare(model, trainer)

# ==============================
# æ˜¾å­˜ä¿¡æ¯
# ==============================
if torch.cuda.is_available():
    print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Allocated: {torch.cuda.memory_allocated(0)/1024**2:.1f} MB")
    print(f"   Reserved:  {torch.cuda.memory_reserved(0)/1024**2:.1f} MB")

# ==============================
# å¯åŠ¨è®­ç»ƒ
# ==============================
print("ğŸš€ Starting QLoRA fine-tuning (offline)...")
trainer.train()
print("ğŸ¯ Training completed successfully!")

# ==============================
# ä¿å­˜æ¨¡å‹
# ==============================
trainer.save_model(output_dir)
print(f"âœ… Model saved to {output_dir}")
