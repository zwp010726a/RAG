#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸€é”®å¼ç¦»çº¿ RAG æµç¨‹ï¼š
- å¤š embedding æ¯”è¾ƒï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
- å›ºå®šç”Ÿæˆæ¨¡å‹ï¼šBaichuan-7Bï¼ˆæœ¬åœ°ï¼‰
- è¾“å‡ºï¼šæ¯æ¨¡å‹ CSVã€æ€» Excel æŠ¥è¡¨ã€è®ºæ–‡çº§å›¾è¡¨
"""

import os
import time
import json
import faiss
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from pathlib import Path
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer

# ================= CONFIG =================
ROOT = Path("/root/Agent")
BAICHUAN_PATH = ROOT / "baichuan-7b"               # ä½ çš„ Baichuan-7B æœ¬åœ°ç›®å½•
RAG_RESULTS_CSV = ROOT / "rag_results.csv"         # æµ‹è¯•é—®é¢˜ï¼ˆquestion,gold_answer,...ï¼‰
DOCUMENTS_CSV = ROOT / "documents.csv"             # æ–‡æ¡£åº“ï¼ŒåŒ…å« doc_id,text åˆ—
OUTPUT_DIR = ROOT / "results"
CHARTS_DIR = OUTPUT_DIR / "charts"

# æœ¬åœ° embedding æ¨¡å‹æ˜ å°„ï¼š key=display name, value=æœ¬åœ°æ¨¡å‹ç›®å½•
EMBED_MODELS = {
    "e5-base-v2": ROOT / "e5-base-v2",
    "bge-large-zh": ROOT / "bge-large-zh",
    "gte-large": ROOT / "gte-large",
}

TOP_K = 5
MAX_CONTEXT_TOKENS = 1500  # ç»™ç”Ÿæˆæ¨¡å‹æ‹¼æ¥çš„ä¸Šä¸‹æ–‡æœ€å¤§ tokenï¼ˆç²—ç•¥æˆªæ–­ï¼‰
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_EMBED = 128
# ==========================================

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHARTS_DIR, exist_ok=True)

# --------- è½½å…¥æµ‹è¯•é›† ----------
if not RAG_RESULTS_CSV.exists():
    raise FileNotFoundError(f"æµ‹è¯•é›†æ–‡ä»¶ {RAG_RESULTS_CSV} ä¸å­˜åœ¨ï¼Œè¯·æä¾› rag_results.csv")

df_test = pd.read_csv(RAG_RESULTS_CSV)
if "question" not in df_test.columns:
    raise KeyError("rag_results.csv éœ€è¦åŒ…å« 'question' åˆ—ï¼ˆä»¥åŠå¯é€‰çš„ gold_answerï¼‰")

questions = df_test["question"].astype(str).tolist()
gold_answers = df_test["gold_answer"].astype(str).tolist() if "gold_answer" in df_test.columns else ["" for _ in questions]

# --------- è½½å…¥æ–‡æ¡£åº“ ----------
if not DOCUMENTS_CSV.exists():
    raise FileNotFoundError(f"documents.csv æœªæ‰¾åˆ° ({DOCUMENTS_CSV})ã€‚è¯·å…ˆç”Ÿæˆæ–‡æ¡£åº“ CSVï¼ˆåŒ…å«åˆ— doc_id,textï¼‰ã€‚")

df_docs = pd.read_csv(DOCUMENTS_CSV)
if "text" not in df_docs.columns:
    raise KeyError("documents.csv å¿…é¡»åŒ…å«åˆ— 'text'")

docs = df_docs["text"].astype(str).tolist()
doc_ids = df_docs["doc_id"].astype(str).tolist() if "doc_id" in df_docs.columns else [str(i) for i in range(len(docs))]

print(f"ğŸ“Œ æ–‡æ¡£åº“åŠ è½½ï¼Œæ–‡æ¡£æ•°ï¼š{len(docs)}")

# --------- è½½å…¥ Baichuan-7B tokenizer + model ----------
print(f"ğŸ¤– åŠ è½½ç”Ÿæˆæ¨¡å‹ï¼ˆBaichuanï¼‰åˆ° {DEVICE} ...")
tokenizer = AutoTokenizer.from_pretrained(str(BAICHUAN_PATH), trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(str(BAICHUAN_PATH), trust_remote_code=True, device_map="auto", torch_dtype=torch.float16)
model.eval()

# å‡†å¤‡è¯„æµ‹å·¥å…·
scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=False)
smooth = SmoothingFunction().method1

def compute_metrics(ref, pred):
    # BLEU (sentence-level, unigram-4 weighted)
    try:
        ref_tok = [ref.split()]
        pred_tok = pred.split()
        bleu = sentence_bleu(ref_tok, pred_tok, smoothing_function=smooth)
    except Exception:
        bleu = 0.0
    # ROUGE-L
    r = scorer.score(ref, pred)
    rouge_l = r["rougeL"].fmeasure
    # EM
    em = 1.0 if ref.strip() == pred.strip() and len(ref.strip())>0 else 0.0
    return bleu, rouge_l, em

# --------- ä¸»æµç¨‹ï¼šå¯¹æ¯ä¸ª embedding æ¨¡å‹åˆ†åˆ«å»ºç«‹ç´¢å¼•ã€æ£€ç´¢ã€ç”Ÿæˆã€è¯„ä¼° ----------
summary_rows = []
all_model_results = {}

for embed_name, embed_dir in EMBED_MODELS.items():
    print(f"\n=== å¤„ç† embedding æ¨¡å‹ï¼š{embed_name} (dir={embed_dir}) ===")
    if not Path(embed_dir).exists():
        print(f"âš ï¸ æœ¬åœ° embedding æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼š{embed_dir} ï¼Œè·³è¿‡è¯¥æ¨¡å‹ã€‚")
        continue

    # åŠ è½½ sentence-transformers
    print("ğŸ” åŠ è½½ SentenceTransformer ...")
    embedder = SentenceTransformer("/root/Agent/e5-base-v2_sbert", device=DEVICE)


    # è®¡ç®—æ–‡æ¡£å‘é‡å¹¶å»ºç«‹ FAISS ç´¢å¼•ï¼ˆå¦‚æœå·²ç¼“å­˜ï¼Œå¯åŠ ç¼“å­˜é€»è¾‘ï¼Œè¿™é‡Œæ¯æ¬¡é‡æ–°è®¡ç®—ï¼‰
    print("ğŸ“¦ è®¡ç®—æ–‡æ¡£å‘é‡å¹¶å»ºç«‹ FAISS ç´¢å¼• ...")
    doc_embeddings = []
    for i in tqdm(range(0, len(docs), BATCH_EMBED), desc="embedding docs"):
        batch_texts = docs[i:i+BATCH_EMBED]
        embs = embedder.encode(batch_texts, convert_to_numpy=True, batch_size=len(batch_texts), show_progress_bar=False, normalize_embeddings=True)
        doc_embeddings.append(embs)
    doc_embeddings = np.vstack(doc_embeddings).astype("float32")
    dim = doc_embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(doc_embeddings)

    # å¯¹æ¯ä¸ªé—®é¢˜æ£€ç´¢ + ç”¨ Baichuan ç”Ÿæˆ
    model_rows = []
    timings = []
    print("ğŸ” å¼€å§‹å¯¹æµ‹è¯•é›†è¿›è¡Œæ£€ç´¢+ç”Ÿæˆ ...")
    for q_idx, question in enumerate(tqdm(questions, desc="questions")):
        start_t = time.time()
        q_emb = embedder.encode([question], convert_to_numpy=True, normalize_embeddings=True)
        D, I = index.search(q_emb.astype("float32"), TOP_K)
        retrieved_texts = [docs[idx] for idx in I[0]]
        retrieved_ids = [doc_ids[idx] for idx in I[0]]

        # æ„é€  promptï¼šæŠŠ Top-K æ–‡æ¡£åˆå¹¶åˆ°ä¸Šä¸‹æ–‡ï¼ˆæŒ‰ç®€å•æ¨¡æ¿ï¼‰
        context = "\n\n".join([f"ã€æ–‡æ¡£_{retrieved_ids[i]}ã€‘ {retrieved_texts[i]}" for i in range(len(retrieved_texts))])
        prompt = f"è¯·æ ¹æ®ä¸‹é¢çš„æ£€ç´¢åˆ°çš„è¯æ®ï¼Œç®€æ´å‡†ç¡®å›ç­”é—®é¢˜ã€‚\n\næ£€ç´¢åˆ°çš„è¯æ®ï¼š\n{context}\n\né—®é¢˜ï¼š{question}\n\nç®€çŸ­å›ç­”ï¼š"

        # é™åˆ¶ prompt é•¿åº¦ï¼ˆç²—ç•¥æˆªæ–­ä¸ºå­—ç¬¦çº§ï¼‰
        if len(prompt) > 16000:
            prompt = prompt[-16000:]  # ä¿ç•™åé¢çš„ä¸€éƒ¨åˆ†ï¼ˆå¦‚è‹¥è¿‡é•¿ï¼Œå¯æ”¹æ›´åˆç†çš„æˆªæ–­ç­–ç•¥ï¼‰

        # tokenize + generate
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(model.device)
        gen_kwargs = dict(max_new_tokens=256, do_sample=True, temperature=0.3, top_p=0.9, repetition_penalty=1.1, eos_token_id=tokenizer.eos_token_id)
        with torch.no_grad():
            out = model.generate(**inputs, **gen_kwargs)
        generated = tokenizer.decode(out[0], skip_special_tokens=True)
        # ç”Ÿæˆæ–‡æœ¬é€šå¸¸åŒ…å« prompt + answerï¼Œå– prompt åé¢çš„å†…å®¹
        if "ç®€çŸ­å›ç­”ï¼š" in generated:
            generated_answer = generated.split("ç®€çŸ­å›ç­”ï¼š")[-1].strip()
        else:
            # å°è¯•æŠŠ prompt çš„æœ€åä¸€éƒ¨åˆ†å»æ‰
            generated_answer = generated[len(tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)):].strip()
        elapsed = time.time() - start_t
        timings.append(elapsed)

        # è¯„ä¼°ï¼ˆæœ‰ gold_answer åˆ™è®¡ç®—ï¼‰
        gold = gold_answers[q_idx] if q_idx < len(gold_answers) else ""
        bleu, rouge_l, em = compute_metrics(gold, generated_answer)

        # æ£€ç´¢å‘½ä¸­ï¼ˆç®€å•ï¼šå‚è€ƒç­”æ¡ˆæ˜¯å¦ä¸ºæ£€ç´¢æ–‡æœ¬å­ä¸²ï¼‰
        hit = any((gold.strip() != "" and gold.strip() in txt) for txt in retrieved_texts)

        row = {
            "question": question,
            "gold_answer": gold,
            "generated_answer": generated_answer,
            "retrieved_ids": "|".join(retrieved_ids),
            "retrieved_texts": " || ".join(retrieved_texts),
            "time_sec": elapsed,
            "bleu": bleu,
            "rouge_l": rouge_l,
            "em": em,
            "retrieval_hit": int(hit),
        }
        model_rows.append(row)

    # æ±‡æ€»å¹¶ä¿å­˜æ¯æ¨¡å‹ CSV
    df_model = pd.DataFrame(model_rows)
    out_csv = OUTPUT_DIR / f"results_{embed_name}.csv"
    df_model.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜æ¯é¢˜ç»“æœï¼š{out_csv} (n={len(df_model)})")

    mean_bleu = df_model["bleu"].mean()
    mean_rouge = df_model["rouge_l"].mean()
    mean_em = df_model["em"].mean()
    mean_time = df_model["time_sec"].mean()
    retrieval_topk_hit = df_model["retrieval_hit"].mean()

    summary_rows.append({
        "embed_model": embed_name,
        "mean_bleu": mean_bleu,
        "mean_rouge_l": mean_rouge,
        "mean_em": mean_em,
        "mean_time_sec": mean_time,
        "retrieval_hit_rate": retrieval_topk_hit,
        "n": len(df_model)
    })

    all_model_results[embed_name] = df_model

# --------- æ±‡æ€»æ‰€æœ‰æ¨¡å‹ç»“æœåˆ° Excel ----------
df_summary = pd.DataFrame(summary_rows)
excel_path = OUTPUT_DIR / "rag_all_results.xlsx"
with pd.ExcelWriter(excel_path, engine="openpyxl") as writer:
    df_summary.to_excel(writer, sheet_name="summary", index=False)
    for name, dfm in all_model_results.items():
        dfm.to_excel(writer, sheet_name=name[:30], index=False)
print(f"\nğŸ“Š æ±‡æ€»å·²ä¿å­˜åˆ° Excel: {excel_path}")

# --------- ç”Ÿæˆè®ºæ–‡å›¾è¡¨ ----------
print("ğŸ“ˆ ç”Ÿæˆè®ºæ–‡ç”¨å›¾è¡¨ ...")
# 1) BLEU/ROUGE æŸ±çŠ¶æ¯”è¾ƒ
plt.figure(figsize=(8,5))
x = df_summary["embed_model"]
x_pos = np.arange(len(x))
plt.bar(x_pos - 0.15, df_summary["mean_bleu"], width=0.3, label="BLEU")
plt.bar(x_pos + 0.15, df_summary["mean_rouge_l"], width=0.3, label="ROUGE-L")
plt.xticks(x_pos, x, rotation=30)
plt.ylabel("score")
plt.title("Embedding model comparison (BLEU / ROUGE-L)")
plt.legend()
plt.tight_layout()
plt.savefig(CHARTS_DIR / "bleu_rouge_comparison.png")
plt.close()

# 2) Time vs BLEU scatter
plt.figure(figsize=(8,5))
plt.scatter(df_summary["mean_time_sec"], df_summary["mean_bleu"])
for i, txt in enumerate(df_summary["embed_model"]):
    plt.annotate(txt, (df_summary["mean_time_sec"].iat[i], df_summary["mean_bleu"].iat[i]))
plt.xlabel("avg gen time (s)")
plt.ylabel("avg BLEU")
plt.title("Time vs BLEU (per embedding model)")
plt.tight_layout()
plt.savefig(CHARTS_DIR / "time_vs_bleu.png")
plt.close()

# 3) retrieval hit rate bar
plt.figure(figsize=(8,5))
plt.bar(df_summary["embed_model"], df_summary["retrieval_hit_rate"])
plt.title(f"Top-{TOP_K} retrieval hit rate (gold included in retrieved text)")
plt.ylabel("hit rate")
plt.tight_layout()
plt.savefig(CHARTS_DIR / "retrieval_hit_rate.png")
plt.close()

print(f"âœ… å›¾è¡¨ä¿å­˜åœ¨ï¼š{CHARTS_DIR}")
print("ğŸ‰ æµç¨‹å®Œæˆã€‚è¯·æŸ¥çœ‹ results/ ä¸‹çš„ Excel/CSV ä¸ charts/ å›¾ç‰‡ï¼Œç”¨äºè®ºæ–‡ç»˜å›¾å’Œè¡¨æ ¼ã€‚")
