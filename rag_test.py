import os
import numpy as np
import faiss
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer

# ========== åŸºç¡€è·¯å¾„è®¾ç½® ==========
EMBED_MODEL_PATH = "/root/Agent/all-MiniLM-L6-v2"
FAISS_INDEX_PATH = "/root/Agent/faiss_index/knowledge.index"
TEXTS_PATH = "/root/Agent/faiss_index/texts.npy"
LLM_PATH = "/root/Agent/baichuan-7b"   # â† ä½ çš„Baichuanæˆ–å…¶ä»–æ¨¡å‹ç›®å½•

# ========== åŠ è½½æ¨¡å‹ ==========
print("ğŸš€ æ­£åœ¨åŠ è½½ SentenceTransformer å‘é‡æ¨¡å‹...")
embedder = SentenceTransformer(EMBED_MODEL_PATH)

print("ğŸ“¦ æ­£åœ¨åŠ è½½ FAISS ç´¢å¼•...")
index = faiss.read_index(FAISS_INDEX_PATH)
texts = np.load(TEXTS_PATH, allow_pickle=True)

print("ğŸ¤– æ­£åœ¨åŠ è½½æœ¬åœ°è¯­è¨€æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(LLM_PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    LLM_PATH,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
).eval()

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# ========== æ„é€  Prompt ==========
def build_prompt(query, retrieved_docs):
    context = "\n".join([f"[{i+1}] {doc}" for i, doc in enumerate(retrieved_docs)])
    prompt = (
        f"ä»¥ä¸‹æ˜¯ä¸é—®é¢˜ç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µï¼Œè¯·ç»“åˆè¿™äº›çŸ¥è¯†ï¼Œç”¨ç®€æ´ã€å‡†ç¡®çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n"
        f"ã€ç›¸å…³çŸ¥è¯†ã€‘\n{context}\n\n"
        f"ã€é—®é¢˜ã€‘{query}\n\n"
        f"ã€å›ç­”ã€‘"
    )
    return prompt


# ========== RAG æŸ¥è¯¢å‡½æ•° ==========
def rag_query(query, top_k=3, max_new_tokens=256):
    print(f"ğŸ§© è¾“å…¥é—®é¢˜ï¼š{query}\n")

    # 1ï¸âƒ£ æ£€ç´¢ç›¸å…³æ–‡æ¡£
    query_vec = embedder.encode([query], normalize_embeddings=True)
    D, I = index.search(query_vec, top_k)
    retrieved = [texts[i] for i in I[0]]

    print("ğŸ” æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µï¼š\n")
    for i, t in enumerate(retrieved, 1):
        print(f"[{i}] {t[:200]}...\n")

    # 2ï¸âƒ£ æ„é€  Prompt
    prompt = build_prompt(query, retrieved)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # 3ï¸âƒ£ æ¨¡å‹ç”Ÿæˆ
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.3,
            top_p=0.9,
            repetition_penalty=1.2,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id
        )

    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    answer = output.split("ã€å›ç­”ã€‘")[-1].strip()

    print("ğŸ’¡ æ¨¡å‹å›ç­”ï¼š\n")
    print(answer)
    print("\n" + "=" * 80 + "\n")

    return answer


# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    print("ğŸ§  RAG é—®ç­”ç³»ç»Ÿå¯åŠ¨æˆåŠŸï¼ï¼ˆBaichuanå…¼å®¹ç‰ˆï¼‰")
    print("è¾“å…¥é—®é¢˜åæŒ‰å›è½¦è·å–å¢å¼ºå›ç­”ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰\n")

    while True:
        query = input("â“è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š").strip()
        if query.lower() in ["exit", "quit", "q"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
        rag_query(query)
